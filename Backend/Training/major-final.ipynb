{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:47:10.592809Z",
     "iopub.status.busy": "2025-01-26T15:47:10.592532Z",
     "iopub.status.idle": "2025-01-26T15:47:15.2505Z",
     "shell.execute_reply": "2025-01-26T15:47:15.249501Z",
     "shell.execute_reply.started": "2025-01-26T15:47:10.592773Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import copy\n",
    "# from datasets import Dataset\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "import pickle\n",
    "import unicodedata\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:47:15.252313Z",
     "iopub.status.busy": "2025-01-26T15:47:15.251842Z",
     "iopub.status.idle": "2025-01-26T15:47:15.263132Z",
     "shell.execute_reply": "2025-01-26T15:47:15.261985Z",
     "shell.execute_reply.started": "2025-01-26T15:47:15.252263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NepaliTokenizer:\n",
    "    def __init__(self):\n",
    "        # Nepali-specific character ranges and rules\n",
    "        self.NEPALI_DEVANAGARI_RANGE = (0x0900, 0x097F)\n",
    "        \n",
    "        # Punctuation and special characters to handle\n",
    "        self.NEPALI_PUNCTUATION = r'।॥,\\.;:!?\\(\\)\\[\\]\\{\\}'\n",
    "        \n",
    "        # Common Nepali suffixes and postpositions to potentially separate\n",
    "        self.NEPALI_SUFFIXES = [\n",
    "            'ले', 'को', 'का', 'की', 'के', \n",
    "            'मा', 'बाट', 'सँग', 'देखि', \n",
    "            'सम्म', 'पछि', 'अघि',\n",
    "            'हरू'  # Plural marker\n",
    "        ]\n",
    "    \n",
    "    def is_nepali_character(self, char):\n",
    "        \"\"\"\n",
    "        Check if a character is in the Devanagari script range used for Nepali\n",
    "        \n",
    "        Args:\n",
    "        - char: Single character to check\n",
    "        \n",
    "        Returns:\n",
    "        - Boolean indicating if character is Nepali\n",
    "        \"\"\"\n",
    "        if not char:\n",
    "            return False\n",
    "        \n",
    "        # Get the Unicode code point of the character\n",
    "        code_point = ord(char)\n",
    "        \n",
    "        # Check if it falls within Devanagari range\n",
    "        return (self.NEPALI_DEVANAGARI_RANGE[0] <= code_point <= self.NEPALI_DEVANAGARI_RANGE[1])\n",
    "    \n",
    "    def normalize_nepali_text(self, text):\n",
    "        \"\"\"\n",
    "        Normalize Nepali text\n",
    "        \n",
    "        Args:\n",
    "        - text: Input text to normalize\n",
    "        \n",
    "        Returns:\n",
    "        - Normalized text\n",
    "        \"\"\"\n",
    "        \n",
    "        # Normalize Unicode decomposition\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "        \n",
    "        # Replace multiple spaces with single space\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Advanced Nepali tokenization method\n",
    "        \n",
    "        Args:\n",
    "        - text: Input text to tokenize\n",
    "        \n",
    "        Returns:\n",
    "        - List of tokens\n",
    "        \"\"\"\n",
    "        # Normalize the text first\n",
    "        text = self.normalize_nepali_text(text)\n",
    "        \n",
    "        # Tokenization strategy\n",
    "        tokens = []\n",
    "        \n",
    "        # Current token being built\n",
    "        current_token = []\n",
    "        \n",
    "        # Iterate through characters\n",
    "        for i, char in enumerate(text):\n",
    "            # Check if character is Nepali, space, or punctuation\n",
    "            if self.is_nepali_character(char):\n",
    "                current_token.append(char)\n",
    "            elif char.isspace():\n",
    "                # If we have a current token, add it\n",
    "                if current_token:\n",
    "                    tokens.append(''.join(current_token))\n",
    "                    current_token = []\n",
    "            elif char in self.NEPALI_PUNCTUATION:\n",
    "                # Add current token if exists\n",
    "                if current_token:\n",
    "                    tokens.append(''.join(current_token))\n",
    "                    current_token = []\n",
    "                \n",
    "                # Add punctuation as separate token\n",
    "                tokens.append(char)\n",
    "            else:\n",
    "                # Non-Nepali characters (like digits, Latin script)\n",
    "                if current_token:\n",
    "                    tokens.append(''.join(current_token))\n",
    "                    current_token = []\n",
    "                tokens.append(char)\n",
    "        \n",
    "        # Add last token if exists\n",
    "        if current_token:\n",
    "            tokens.append(''.join(current_token))\n",
    "        \n",
    "        # Suffix and postposition handling\n",
    "        final_tokens = []\n",
    "        for token in tokens:\n",
    "            # Check for tokens that can be further split\n",
    "            if self.is_nepali_character(token[-1]):\n",
    "                # Check for known suffixes\n",
    "                for suffix in self.NEPALI_SUFFIXES:\n",
    "                    if token.endswith(suffix):\n",
    "                        base_word = token[:-len(suffix)]\n",
    "                        if base_word:\n",
    "                            final_tokens.append(base_word)\n",
    "                            final_tokens.append(suffix)\n",
    "                            break\n",
    "                else:\n",
    "                    final_tokens.append(token)\n",
    "            else:\n",
    "                final_tokens.append(token)\n",
    "        \n",
    "        return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:47:15.265159Z",
     "iopub.status.busy": "2025-01-26T15:47:15.26485Z",
     "iopub.status.idle": "2025-01-26T15:47:15.297773Z",
     "shell.execute_reply": "2025-01-26T15:47:15.296715Z",
     "shell.execute_reply.started": "2025-01-26T15:47:15.265133Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomBERTTokenizer:\n",
    "    def __init__(self, \n",
    "                 max_vocab_size=30000, \n",
    "                 max_length=512, \n",
    "                 mask_probability=0.15):\n",
    "        \"\"\"\n",
    "        Custom BERT-style tokenizer\n",
    "        \n",
    "        Args:\n",
    "        - max_vocab_size: Maximum number of tokens in vocabulary\n",
    "        - max_length: Maximum sequence length\n",
    "        - mask_probability: Probability of masking a token\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.mask_probability = mask_probability\n",
    "        \n",
    "        # Special tokens\n",
    "        self.special_tokens = {\n",
    "            '[PAD]': 0,\n",
    "            '[UNK]': 1,\n",
    "            '[CLS]': 2,\n",
    "            '[SEP]': 3,\n",
    "            '[MASK]': 4\n",
    "        }\n",
    "        self.special_token_ids = {token: idx for token, idx in self.special_tokens.items()}\n",
    "        \n",
    "        # Vocabulary will be built dynamically\n",
    "        self.vocab = self.special_tokens.copy()\n",
    "        self.reverse_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        \n",
    "        # Keep track of token frequencies\n",
    "        self.token_freq = Counter()\n",
    "        \n",
    "        # Tokenization parameters\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.tokenizer = NepaliTokenizer()\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Basic tokenization method\n",
    "        \n",
    "        Args:\n",
    "        - text: Input text to tokenize\n",
    "        \n",
    "        Returns:\n",
    "        - List of tokens\n",
    "        \"\"\"\n",
    "\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"\n",
    "        Build vocabulary from corpus\n",
    "        \n",
    "        Args:\n",
    "        - texts: List of texts to build vocabulary from\n",
    "        \"\"\"\n",
    "        # Tokenize all texts\n",
    "        all_tokens = []\n",
    "        for text in texts:\n",
    "            tokens = self._tokenize(text)\n",
    "            all_tokens.extend(tokens)\n",
    "            self.token_freq.update(tokens)\n",
    "        \n",
    "        # Sort tokens by frequency\n",
    "        sorted_tokens = sorted(\n",
    "            self.token_freq.items(), \n",
    "            key=lambda x: x[1], \n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Add most frequent tokens to vocabulary\n",
    "        next_idx = max(self.special_token_ids.values()) + 1\n",
    "        for token, _ in sorted_tokens:\n",
    "            if token not in self.vocab:\n",
    "                self.vocab[token] = next_idx\n",
    "                self.reverse_vocab[next_idx] = token\n",
    "                next_idx += 1\n",
    "                \n",
    "                # Stop if we reach max vocab size\n",
    "                if len(self.vocab) >= self.max_vocab_size:\n",
    "                    break\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode text to token ids\n",
    "        \n",
    "        Args:\n",
    "        - text: Input text\n",
    "        \n",
    "        Returns:\n",
    "        - List of token ids\n",
    "        \"\"\"\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = self._tokenize(text)\n",
    "    \n",
    "        # Convert to ids, using [UNK] for out-of-vocab tokens\n",
    "        token_ids = [\n",
    "            self.vocab.get(token, self.special_token_ids['[UNK]'])\n",
    "            for token in tokens\n",
    "        ]\n",
    "    \n",
    "        # Add special tokens\n",
    "        token_ids = [self.special_token_ids['[CLS]']] + \\\n",
    "                    token_ids + \\\n",
    "                    [self.special_token_ids['[SEP]']]\n",
    "    \n",
    "        # Truncate or pad to max_length\n",
    "        token_ids = token_ids[:self.max_length]\n",
    "        token_ids += [self.special_token_ids['[PAD]']] * (self.max_length - len(token_ids))\n",
    "    \n",
    "        return token_ids\n",
    "\n",
    "    def mask_tokens(self, input_ids):\n",
    "        \"\"\"\n",
    "        Apply token masking\n",
    "        \n",
    "        Args:\n",
    "        - input_ids: Original token sequence\n",
    "        \n",
    "        Returns:\n",
    "        - masked_input_ids: Input with some tokens masked\n",
    "        - mask_labels: Original tokens before masking\n",
    "        \"\"\"\n",
    "        # Ensure input_ids is a torch tensor\n",
    "        if not isinstance(input_ids, torch.Tensor):\n",
    "            input_ids = torch.tensor(input_ids)\n",
    "        \n",
    "        # Create a copy of input_ids\n",
    "        masked_input_ids = input_ids.clone()\n",
    "        \n",
    "        # Create mask for tokens to be masked (excluding special tokens)\n",
    "        mask = torch.bernoulli(torch.full(masked_input_ids.shape, self.mask_probability)).bool()\n",
    "        mask &= (masked_input_ids != self.special_token_ids['[CLS]']) & \\\n",
    "               (masked_input_ids != self.special_token_ids['[SEP]']) & \\\n",
    "               (masked_input_ids != self.special_token_ids['[PAD]'])\n",
    "        \n",
    "        # If no tokens are masked, randomly mask at least one\n",
    "        if not mask.any():\n",
    "            # Randomly select a non-special token to mask\n",
    "            non_special_mask = (masked_input_ids != self.special_token_ids['[CLS]']) & \\\n",
    "                               (masked_input_ids != self.special_token_ids['[SEP]']) & \\\n",
    "                               (masked_input_ids != self.special_token_ids['[PAD]'])\n",
    "            if non_special_mask.any():\n",
    "                random_index = torch.multinomial(non_special_mask.float(), 1)[0]\n",
    "                mask[random_index] = True\n",
    "        \n",
    "        # Create labels for masked tokens\n",
    "        mask_labels = torch.zeros_like(masked_input_ids)\n",
    "        mask_labels[mask] = masked_input_ids[mask]\n",
    "        \n",
    "        # 80% of masked tokens are replaced with [MASK]\n",
    "        mask_mask = mask & (torch.rand_like(masked_input_ids.float()) < 0.8)\n",
    "        masked_input_ids[mask_mask] = self.special_token_ids['[MASK]']\n",
    "        \n",
    "        # 10% of masked tokens are replaced with random tokens\n",
    "        if mask.any():\n",
    "            random_tokens = torch.randint_like(\n",
    "                masked_input_ids, \n",
    "                0, \n",
    "                len(self.vocab)\n",
    "            )\n",
    "            random_mask = mask & (torch.rand_like(masked_input_ids.float()) < 0.1)\n",
    "            masked_input_ids[random_mask] = random_tokens[random_mask]\n",
    "        \n",
    "        return masked_input_ids, mask_labels\n",
    "\n",
    "    def prepare_bert_pretraining_data(self, df, text_column):\n",
    "        \"\"\"\n",
    "        Prepare BERT pretraining data from a DataFrame\n",
    "        \n",
    "        Args:\n",
    "        - df: Input DataFrame\n",
    "        - text_column: Name of the text column\n",
    "        \n",
    "        Returns:\n",
    "        - Tuple of tensors for pretraining, now with masked_tokens\n",
    "        \"\"\"\n",
    "        # First, build vocabulary\n",
    "        # self.build_vocab(df[text_column])\n",
    "        \n",
    "        # Prepare lists to store data\n",
    "        input_sequences = []\n",
    "        segment_ids = []\n",
    "        masked_tokens = []\n",
    "        \n",
    "        # Iterate through the DataFrame\n",
    "        for i in range(len(df)):\n",
    "            try:\n",
    "                text1 = df[text_column].iloc[i]\n",
    "                \n",
    "                input_ids1 = self.encode(text1)\n",
    "                \n",
    "                # Combine texts with segment ids\n",
    "                combined_input_ids = input_ids1\n",
    "                segment_ids_tensor = torch.zeros(len(combined_input_ids), dtype=torch.long)\n",
    "                segment_ids_tensor[len(input_ids1):] = 1\n",
    "\n",
    "                #AFTER GPU ON\n",
    "                masked_input_ids, mask_label = self.mask_tokens(combined_input_ids)\n",
    "                # Append to lists\n",
    "                input_sequences.append(torch.tensor(combined_input_ids).clone().detach())\n",
    "                segment_ids.append(segment_ids_tensor.clone().detach())\n",
    "                masked_tokens.append(mask_label.clone().detach())\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Convert to tensors\n",
    "        input_sequences = torch.stack(input_sequences)\n",
    "        segment_ids = torch.stack(segment_ids)\n",
    "        masked_tokens = torch.stack(masked_tokens)\n",
    "        \n",
    "        return input_sequences, segment_ids, masked_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:47:15.299124Z",
     "iopub.status.busy": "2025-01-26T15:47:15.298785Z",
     "iopub.status.idle": "2025-01-26T15:47:15.316483Z",
     "shell.execute_reply": "2025-01-26T15:47:15.315411Z",
     "shell.execute_reply.started": "2025-01-26T15:47:15.299089Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PretrainingDataset(Dataset):\n",
    "    def __init__(self, input_sequences, segment_ids, masked_tokens):\n",
    "        self.input_sequences = input_sequences\n",
    "        self.segment_ids = segment_ids\n",
    "        self.masked_tokens = masked_tokens\n",
    "    def __len__(self):\n",
    "        return len(self.input_sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_sequences':torch.tensor(self.input_sequences[idx], dtype=torch.long),\n",
    "            'segment_ids':torch.tensor(self.segment_ids[idx], dtype=torch.long),\n",
    "            'masked_tokens':torch.tensor(self.masked_tokens[idx],  dtype=torch.long)\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:47:15.319192Z",
     "iopub.status.busy": "2025-01-26T15:47:15.318935Z",
     "iopub.status.idle": "2025-01-26T15:47:15.333467Z",
     "shell.execute_reply": "2025-01-26T15:47:15.332411Z",
     "shell.execute_reply.started": "2025-01-26T15:47:15.319171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_total_vocabulary(file_path, tokenizer, chunk_size=1000, max_chunks=None):\n",
    "    \"\"\"\n",
    "    Build vocabulary from entire dataset before processing chunks\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to TSV file\n",
    "        tokenizer: CustomBERTTokenizer instance\n",
    "        chunk_size: Number of rows to process at once\n",
    "        max_chunks: Maximum number of chunks to process (None for all)\n",
    "    \"\"\"\n",
    "    # Create chunk iterator for vocabulary building\n",
    "    chunks = pd.read_csv(\n",
    "        file_path, \n",
    "        sep='\\t', \n",
    "        header=None, \n",
    "        names=['text'], \n",
    "        chunksize=chunk_size,\n",
    "        encoding='utf-8'\n",
    "    )\n",
    "    \n",
    "    print(\"Building vocabulary from all chunks...\")\n",
    "    for chunk_idx, chunk in enumerate(tqdm(chunks, desc=\"Processing chunks for vocabulary\")):\n",
    "        if max_chunks and chunk_idx >= max_chunks:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            # Build vocabulary from this chunk\n",
    "            tokenizer.build_vocab(chunk['text'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {chunk_idx} for vocabulary: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Built vocabulary with {len(tokenizer.vocab)} tokens\")\n",
    "    return tokenizer\n",
    "\n",
    "def save_vocabulary(tokenizer, output_path):\n",
    "    \"\"\"\n",
    "    Save tokenizer vocabulary to file\n",
    "    \"\"\"\n",
    "    vocab_path = Path(output_path).parent / \"vocabulary.json\"\n",
    "    \n",
    "    vocab_data = {\n",
    "        'vocab': tokenizer.vocab,\n",
    "        'special_tokens': tokenizer.special_tokens,\n",
    "        'max_vocab_size': tokenizer.max_vocab_size,\n",
    "        'max_length': tokenizer.max_length,\n",
    "        'mask_probability': tokenizer.mask_probability\n",
    "    }\n",
    "    \n",
    "    with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(vocab_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return vocab_path\n",
    "\n",
    "def load_vocabulary(vocab_path):\n",
    "    \"\"\"\n",
    "    Load saved vocabulary and initialize tokenizer\n",
    "    \"\"\"\n",
    "    with open(vocab_path, 'r', encoding='utf-8') as f:\n",
    "        vocab_data = json.load(f)\n",
    "    \n",
    "    tokenizer = CustomBERTTokenizer(\n",
    "        max_vocab_size=vocab_data['max_vocab_size'],\n",
    "        max_length=vocab_data['max_length'],\n",
    "        mask_probability=vocab_data['mask_probability']\n",
    "    )\n",
    "    \n",
    "    tokenizer.vocab = vocab_data['vocab']\n",
    "    tokenizer.special_tokens = vocab_data['special_tokens']\n",
    "    tokenizer.reverse_vocab = {v: k for k, v in tokenizer.vocab.items()}\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:47:15.335573Z",
     "iopub.status.busy": "2025-01-26T15:47:15.335156Z",
     "iopub.status.idle": "2025-01-26T15:47:15.353626Z",
     "shell.execute_reply": "2025-01-26T15:47:15.352597Z",
     "shell.execute_reply.started": "2025-01-26T15:47:15.33553Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class ChunkDataset(Dataset):\n",
    "#     \"\"\"Dataset that loads chunks on-demand\"\"\"\n",
    "    \n",
    "#     def __init__(self, metadata_path):\n",
    "#         \"\"\"\n",
    "#         Initialize dataset using metadata file\n",
    "        \n",
    "#         Args:\n",
    "#             metadata_path: Path to metadata JSON file\n",
    "#         \"\"\"\n",
    "#         with open(metadata_path) as f:\n",
    "#             self.metadata = json.load(f)\n",
    "        \n",
    "#         self.chunks = self.metadata['chunks']\n",
    "#         # self.format = self.metadata['format']\n",
    "        \n",
    "#         # Load vocabulary\n",
    "#         self.tokenizer = load_vocabulary(self.metadata['vocab_path'])\n",
    "        \n",
    "#         # Calculate cumulative sizes for chunk lookup\n",
    "#         self.cumulative_sizes = np.cumsum([chunk['num_sequences'] for chunk in self.chunks])\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return self.cumulative_sizes[-1]\n",
    "    \n",
    "#     def _find_chunk(self, idx):\n",
    "#         \"\"\"Find which chunk contains the given index\"\"\"\n",
    "#         chunk_idx = np.searchsorted(self.cumulative_sizes, idx, side='right')\n",
    "#         local_idx = idx - (self.cumulative_sizes[chunk_idx-1] if chunk_idx > 0 else 0)\n",
    "#         return chunk_idx, local_idx\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         chunk_idx, local_idx = self._find_chunk(idx)\n",
    "#         chunk_path = self.chunks[chunk_idx]['path']\n",
    "        \n",
    "#         # Load appropriate slice from chunk file\n",
    "#         if self.metadata['format'] == 'h5':\n",
    "#             with h5py.File(chunk_path, 'r') as f:\n",
    "#                 return {\n",
    "#                     'input_sequences': torch.tensor(f['input_sequences'][local_idx], dtype=torch.long),\n",
    "#                     'segment_ids': torch.tensor(f['segment_ids'][local_idx], dtype=torch.long),\n",
    "#                     'masked_tokens': torch.tensor(f['masked_tokens'][local_idx], dtype=torch.long)\n",
    "#                 }\n",
    "#         else:  # npz\n",
    "#             data = np.load(chunk_path)\n",
    "#             return {\n",
    "#                 'input_sequences': torch.tensor(data['input_sequences'][local_idx], dtype=torch.long),\n",
    "#                 'segment_ids': torch.tensor(data['segment_ids'][local_idx], dtype=torch.long),\n",
    "#                 'masked_tokens': torch.tensor(data['masked_tokens'][local_idx], dtype=torch.long)\n",
    "#             }\n",
    "    # def split(self, train_ratio=0.8, valid_ratio=0.1, seed=42):\n",
    "    #     \"\"\"\n",
    "    #     Split dataset into train, validation, and test sets\n",
    "        \n",
    "    #     Args:\n",
    "    #         train_ratio: Proportion of data for training\n",
    "    #         valid_ratio: Proportion of data for validation \n",
    "    #         seed: Random seed for reproducibility\n",
    "        \n",
    "    #     Returns:\n",
    "    #         Tuple of (train_dataset, valid_dataset, test_dataset)\n",
    "    #     \"\"\"\n",
    "    #     # Total dataset size\n",
    "    #     total_size = len(self)\n",
    "        \n",
    "    #     # Set random seed\n",
    "    #     np.random.seed(seed)\n",
    "        \n",
    "    #     # Generate random indices\n",
    "    #     indices = np.random.permutation(total_size)\n",
    "        \n",
    "    #     # Calculate split points\n",
    "    #     train_end = int(total_size * train_ratio)\n",
    "    #     valid_end = train_end + int(total_size * valid_ratio)\n",
    "        \n",
    "    #     # Create train split datasets\n",
    "    #     train_indices = indices[:train_end]\n",
    "    #     valid_indices = indices[train_end:valid_end]\n",
    "    #     test_indices = indices[valid_end:]\n",
    "        \n",
    "    #     def create_split_dataset(subset_indices):\n",
    "    #         \"\"\"Create a subset dataset from given indices\"\"\"\n",
    "    #         split_dataset = copy.deepcopy(self)\n",
    "    #         split_dataset.indices = subset_indices\n",
    "    #         split_dataset.__getitem__ = lambda idx: self.__getitem__(subset_indices[idx])\n",
    "    #         split_dataset.__len__ = lambda: len(subset_indices)\n",
    "    #         return split_dataset\n",
    "        \n",
    "    #     return (\n",
    "    #         create_split_dataset(train_indices),\n",
    "    #         create_split_dataset(valid_indices),\n",
    "    #         create_split_dataset(test_indices)\n",
    "    #     )\n",
    "\n",
    "class ChunkDataset(Dataset):\n",
    "    \"\"\"Dataset that loads chunks on-demand with support for multiple indices and slicing\"\"\"\n",
    "    \n",
    "    def __init__(self, metadata_path):\n",
    "        \"\"\"\n",
    "        Initialize dataset using metadata file\n",
    "        \n",
    "        Args:\n",
    "            metadata_path: Path to metadata JSON file\n",
    "        \"\"\"\n",
    "        with open(metadata_path) as f:\n",
    "            self.metadata = json.load(f)\n",
    "        \n",
    "        self.chunks = self.metadata['chunks']\n",
    "        \n",
    "        # Load vocabulary\n",
    "        self.tokenizer = load_vocabulary(self.metadata['vocab_path'])\n",
    "        \n",
    "        # Calculate cumulative sizes for chunk lookup\n",
    "        self.cumulative_sizes = np.cumsum([chunk['num_sequences'] for chunk in self.chunks])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.cumulative_sizes[-1]\n",
    "    \n",
    "    def _find_chunk(self, idx):\n",
    "        \"\"\"Find which chunk contains the given index\"\"\"\n",
    "        chunk_idx = np.searchsorted(self.cumulative_sizes, idx, side='right')\n",
    "        local_idx = idx - (self.cumulative_sizes[chunk_idx-1] if chunk_idx > 0 else 0)\n",
    "        return chunk_idx, local_idx\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        # Handle slicing\n",
    "        if isinstance(key, slice):\n",
    "            start = key.start or 0\n",
    "            stop = key.stop or len(self)\n",
    "            step = key.step or 1\n",
    "            \n",
    "            # Convert slice to list of indices\n",
    "            indices = range(start, stop, step)\n",
    "            return [self[idx] for idx in indices]\n",
    "        \n",
    "        # Handle single index\n",
    "        if isinstance(key, int):\n",
    "            # Normalize negative indices\n",
    "            if key < 0:\n",
    "                key += len(self)\n",
    "            \n",
    "            # Validate index\n",
    "            if key < 0 or key >= len(self):\n",
    "                raise IndexError(\"Index out of range\")\n",
    "            \n",
    "            chunk_idx, local_idx = self._find_chunk(key)\n",
    "            chunk_path = self.chunks[chunk_idx]['path']\n",
    "            \n",
    "            # Load appropriate slice from chunk file\n",
    "            if self.metadata['format'] == 'h5':\n",
    "                with h5py.File(chunk_path, 'r') as f:\n",
    "                    return {\n",
    "                        'input_sequences': torch.tensor(f['input_sequences'][local_idx], dtype=torch.long),\n",
    "                        'segment_ids': torch.tensor(f['segment_ids'][local_idx], dtype=torch.long),\n",
    "                        'masked_tokens': torch.tensor(f['masked_tokens'][local_idx], dtype=torch.long)\n",
    "                    }\n",
    "            else:  # npz\n",
    "                data = np.load(chunk_path)\n",
    "                return {\n",
    "                    'input_sequences': torch.tensor(data['input_sequences'][local_idx], dtype=torch.long),\n",
    "                    'segment_ids': torch.tensor(data['segment_ids'][local_idx], dtype=torch.long),\n",
    "                    'masked_tokens': torch.tensor(data['masked_tokens'][local_idx], dtype=torch.long)\n",
    "                }\n",
    "        \n",
    "        # Handle list/array of indices\n",
    "        if isinstance(key, (list, np.ndarray)):\n",
    "            return [self[idx] for idx in key]\n",
    "        \n",
    "        raise TypeError(\"Invalid argument type\")\n",
    "\n",
    "    \n",
    "    def split(self, train_ratio=0.8, valid_ratio=0.1, seed=42):\n",
    "        \"\"\"\n",
    "        Split dataset into train, validation, and test sets\n",
    "        \n",
    "        Args:\n",
    "            train_ratio: Proportion of data for training\n",
    "            valid_ratio: Proportion of data for validation \n",
    "            seed: Random seed for reproducibility\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (train_dataset, valid_dataset, test_dataset)\n",
    "        \"\"\"\n",
    "        # Total dataset size\n",
    "        total_size = len(self)\n",
    "        \n",
    "        # Set random seed\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Generate random indices\n",
    "        indices = np.random.permutation(total_size)\n",
    "        \n",
    "        # Calculate split points\n",
    "        train_end = int(total_size * train_ratio)\n",
    "        valid_end = train_end + int(total_size * valid_ratio)\n",
    "        \n",
    "        # Create train split datasets\n",
    "        train_indices = indices[:train_end]\n",
    "        valid_indices = indices[train_end:valid_end]\n",
    "        test_indices = indices[valid_end:]\n",
    "        \n",
    "        def create_split_dataset(subset_indices):\n",
    "            \"\"\"Create a subset dataset from given indices\"\"\"\n",
    "            split_dataset = copy.deepcopy(self)\n",
    "            split_dataset.indices = subset_indices\n",
    "            split_dataset.__getitem__ = lambda idx: self.__getitem__(subset_indices[idx])\n",
    "            split_dataset.__len__ = lambda: len(subset_indices)\n",
    "            return split_dataset\n",
    "        \n",
    "        return (\n",
    "            create_split_dataset(train_indices),\n",
    "            create_split_dataset(valid_indices),\n",
    "            create_split_dataset(test_indices)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:47:15.355093Z",
     "iopub.status.busy": "2025-01-26T15:47:15.354671Z",
     "iopub.status.idle": "2025-01-26T15:47:15.809092Z",
     "shell.execute_reply": "2025-01-26T15:47:15.808133Z",
     "shell.execute_reply.started": "2025-01-26T15:47:15.355058Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = ChunkDataset('/home/ubuntu/dataset/processed_chunks_metadata.json')\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "train_dataset, valid_dataset, test_dataset = dataset.split(\n",
    "    train_ratio=0.8,  # 80% training\n",
    "    valid_ratio=0.1,  # 10% validation\n",
    "    seed=42  # For reproducibility\n",
    ")\n",
    "# dataloader = DataLoader(dataset,batch_size=32,shuffle=True)\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64,num_workers=16, shuffle=True,pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=64,num_workers=16,pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64,num_workers=16,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:47:15.81144Z",
     "iopub.status.busy": "2025-01-26T15:47:15.811087Z",
     "iopub.status.idle": "2025-01-26T15:47:15.815373Z",
     "shell.execute_reply": "2025-01-26T15:47:15.814343Z",
     "shell.execute_reply.started": "2025-01-26T15:47:15.811414Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True,pin_memory=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True,pin_memory=True)\n",
    "# valid_dataloader = DataLoader(valid_dataset,batch_size=8,shuffle=True,pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:47:15.816746Z",
     "iopub.status.busy": "2025-01-26T15:47:15.816495Z",
     "iopub.status.idle": "2025-01-26T15:47:15.832897Z",
     "shell.execute_reply": "2025-01-26T15:47:15.831785Z",
     "shell.execute_reply.started": "2025-01-26T15:47:15.816724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, n_segments, max_len, embed_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, embed_dim)  # Token embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, embed_dim)  # Segment embedding\n",
    "        self.pos_embed = nn.Embedding(max_len, embed_dim)     # Positional embedding\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.max_len = max_len  # Store max length for positional embedding\n",
    "\n",
    "    def forward(self, seq, seg):\n",
    "        # Dynamically generate position indices on the same device as `seq`\n",
    "        pos_inp = torch.arange(seq.size(1), device=seq.device).unsqueeze(0).expand_as(seq)\n",
    "        embed_val = self.tok_embed(seq) + self.seg_embed(seg) + self.pos_embed(pos_inp)\n",
    "        embed_val = self.drop(embed_val)\n",
    "        return embed_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:47:15.834214Z",
     "iopub.status.busy": "2025-01-26T15:47:15.833903Z",
     "iopub.status.idle": "2025-01-26T15:47:15.851677Z",
     "shell.execute_reply": "2025-01-26T15:47:15.850651Z",
     "shell.execute_reply.started": "2025-01-26T15:47:15.834185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 n_segments,\n",
    "                 max_len,\n",
    "                 embed_dim,\n",
    "                 n_layers,\n",
    "                 attn_heads,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = BERTEmbedding(vocab_size, n_segments, max_len, embed_dim, dropout)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(embed_dim, attn_heads, embed_dim*4)\n",
    "        self.encoder_block = nn.TransformerEncoder(self.encoder_layer, n_layers)\n",
    "    def forward(self, seq, seg):\n",
    "        out = self.embedding(seq, seg)\n",
    "        out = self.encoder_block(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:47:15.852903Z",
     "iopub.status.busy": "2025-01-26T15:47:15.852591Z",
     "iopub.status.idle": "2025-01-26T15:47:15.866634Z",
     "shell.execute_reply": "2025-01-26T15:47:15.865694Z",
     "shell.execute_reply.started": "2025-01-26T15:47:15.852872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BERTPretrainingModel(nn.Module):\n",
    "    def __init__(self, bert_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        # MLM head\n",
    "        self.mlm_head = nn.Linear(\n",
    "            bert_model.embedding.tok_embed.embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, seq, seg):\n",
    "        # Get BERT embeddings\n",
    "        bert_output = self.bert(seq, seg)  # [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        # MLM prediction for all tokens\n",
    "        mlm_prediction = self.mlm_head(bert_output)  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        return mlm_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:47:15.86797Z",
     "iopub.status.busy": "2025-01-26T15:47:15.867703Z",
     "iopub.status.idle": "2025-01-26T15:47:15.945933Z",
     "shell.execute_reply": "2025-01-26T15:47:15.94466Z",
     "shell.execute_reply.started": "2025-01-26T15:47:15.867948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def train_bert(model, train_dataloader, optimizer, mlm_criterion, device):\n",
    "#     model.train()\n",
    "#     total_train_loss = 0\n",
    "\n",
    "#     for batch in tqdm(train_dataloader):\n",
    "#         seq = batch['input_sequences'].to(device)  # [batch_size, seq_len]\n",
    "#         seg = batch['segment_ids'].to(device)      # [batch_size, seq_len]\n",
    "#         masked_tokens = batch['masked_tokens'].to(device)  # [batch_size, seq_len]\n",
    "\n",
    "#         # Zero gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         mlm_predictions = model(seq, seg)  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "#         # Flatten predictions and targets\n",
    "#         mlm_predictions = mlm_predictions.view(-1, mlm_predictions.size(-1))  # [batch_size * seq_len, vocab_size]\n",
    "#         masked_tokens = masked_tokens.view(-1)  # [batch_size * seq_len]\n",
    "\n",
    "#         # Compute MLM loss\n",
    "#         mlm_loss = mlm_criterion(mlm_predictions, masked_tokens)\n",
    "\n",
    "#         # Backward pass\n",
    "#         mlm_loss.backward()\n",
    "\n",
    "#         # Optimizer step\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Accumulate loss\n",
    "#         total_train_loss += mlm_loss.item()\n",
    "\n",
    "#     return total_train_loss / len(train_dataloader)\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "def train_bert(model, train_dataloader, optimizer, mlm_criterion, device, train_batch_loss, patience=50):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    flag = False\n",
    "    train_bar = tqdm(train_dataloader)\n",
    "    \n",
    "    best_loss = float(\"inf\")\n",
    "    batches_no_improve = 0  \n",
    "\n",
    "    for batch in train_bar:\n",
    "        seq = batch['input_sequences'].to(device)\n",
    "        seg = batch['segment_ids'].to(device)\n",
    "        masked_tokens = batch['masked_tokens'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            mlm_predictions = model(seq, seg)\n",
    "            mlm_predictions = mlm_predictions.view(-1, mlm_predictions.size(-1))\n",
    "            masked_tokens = masked_tokens.view(-1)\n",
    "            mlm_loss = mlm_criterion(mlm_predictions, masked_tokens)\n",
    "\n",
    "        train_batch_loss.append(mlm_loss.item())\n",
    "        scaler.scale(mlm_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_bar.set_postfix({\"loss\":mlm_loss.item()})\n",
    "        total_train_loss += mlm_loss.item()\n",
    "\n",
    "        if mlm_loss.item() < best_loss:\n",
    "            best_loss = mlm_loss.item()\n",
    "            batches_no_improve = 0  \n",
    "        else:\n",
    "            batches_no_improve += 1\n",
    "        \n",
    "        if batches_no_improve >= patience:\n",
    "            flag = True  \n",
    "\n",
    "    return total_train_loss / len(train_dataloader),flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T15:47:15.947359Z",
     "iopub.status.busy": "2025-01-26T15:47:15.946964Z",
     "iopub.status.idle": "2025-01-26T15:47:15.962394Z",
     "shell.execute_reply": "2025-01-26T15:47:15.961399Z",
     "shell.execute_reply.started": "2025-01-26T15:47:15.947319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_bert(model, test_dataloader, mlm_criterion, device,valid_batch_loss):\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    test_bar = tqdm(test_dataloader)\n",
    "    \n",
    "    for batch in test_bar:\n",
    "        seq = batch['input_sequences'].to(device)\n",
    "        seg = batch['segment_ids'].to(device)\n",
    "        masked_tokens = batch['masked_tokens'].to(device)\n",
    "\n",
    "        with autocast():\n",
    "            mlm_predictions = model(seq, seg)\n",
    "            mlm_predictions = mlm_predictions.view(-1, mlm_predictions.size(-1))\n",
    "            masked_tokens = masked_tokens.view(-1)\n",
    "            mlm_loss = mlm_criterion(mlm_predictions, masked_tokens)\n",
    "\n",
    "        valid_batch_loss.append(mlm_loss.item())\n",
    "        total_test_loss += mlm_loss.item()\n",
    "        test_bar.set_postfix({\"loss\": mlm_loss.item()})\n",
    "\n",
    "    return total_test_loss / len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpointing \n",
    "scaler = GradScaler()\n",
    "checkpoint_path = \"checkpoint.pth\"\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, path=checkpoint_path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scaler_state_dict': scaler.state_dict()\n",
    "    }, path)\n",
    "\n",
    "def load_checkpoint(model, optimizer, path=checkpoint_path):\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        return checkpoint['epoch'] + 1  # Resume from next epoch\n",
    "    return 0  # Start fresh if no checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-26T15:49:54.571Z",
     "iopub.execute_input": "2025-01-26T15:47:15.966008Z",
     "iopub.status.busy": "2025-01-26T15:47:15.965621Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65589e3775414f81a6c03ce9fd3f8de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69913 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, num_epochs):\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m     train_mlm_loss,flag \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_bert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlm_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_batch_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain MLM Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_mlm_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mappend(train_mlm_loss)\n",
      "Cell \u001b[0;32mIn[12], line 48\u001b[0m, in \u001b[0;36mtrain_bert\u001b[0;34m(model, train_dataloader, optimizer, mlm_criterion, device, train_batch_loss, patience)\u001b[0m\n\u001b[1;32m     45\u001b[0m batches_no_improve \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_bar:\n\u001b[0;32m---> 48\u001b[0m     seq \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_sequences\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     seg \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegment_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     50\u001b[0m     masked_tokens \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasked_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = 30000 \n",
    "n_segments = 2\n",
    "max_len = 512\n",
    "embed_dim = 768\n",
    "n_layers = 6\n",
    "attn_heads = 6\n",
    "dropout = 0.1\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize BERT model\n",
    "# bert_base = BERT(\n",
    "#     vocab_size=vocab_size, \n",
    "#     n_segments=n_segments, \n",
    "#     max_len=max_len, \n",
    "#     embed_dim=embed_dim, \n",
    "#     n_layers=n_layers, \n",
    "#     attn_heads=attn_heads, \n",
    "#     dropout=dropout\n",
    "# ).to(device)\n",
    "\n",
    "bert_base = torch.compile(BERT(\n",
    "    vocab_size=vocab_size, \n",
    "    n_segments=n_segments, \n",
    "    max_len=max_len, \n",
    "    embed_dim=embed_dim, \n",
    "    n_layers=n_layers, \n",
    "    attn_heads=attn_heads, \n",
    "    dropout=dropout\n",
    ").to(device))\n",
    "\n",
    "# Wrap BERT in pretraining model\n",
    "# model = BERTPretrainingModel(bert_base, vocab_size).to(device)\n",
    "\n",
    "model = torch.compile(BERTPretrainingModel(bert_base, vocab_size).to(device))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Loss functions\n",
    "mlm_criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_batch_losses = []\n",
    "valid_batch_losses = []\n",
    "\n",
    "start_epoch = load_checkpoint(model, optimizer)\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    train_mlm_loss,flag = train_bert(model, train_dataloader, optimizer, mlm_criterion, device,train_batch_losses,patience=50)\n",
    "    print(\"Train MLM Loss: {train_mlm_loss}\")\n",
    "    train_loss.append(train_mlm_loss)\n",
    "    \n",
    "    valid_mlm_loss = test_bert(model, valid_dataloader, mlm_criterion, device,valid_batch_losses)\n",
    "    print(\"Test MLM Loss: {valid_mlm_loss}\")\n",
    "    valid_loss.append(valid_mlm_loss)\n",
    "    \n",
    "    save_checkpoint(model, optimizer, epoch)\n",
    "\n",
    "    if flag == True:\n",
    "        print(f\"Epoch:{epoch} early stopping triggered\")\n",
    "    \n",
    "# Save the model\n",
    "torch.save(model, 'model.pth')\n",
    "torch.save(model.state_dict(), 'model_state_dict.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-26T15:49:54.617Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_mlm_loss = test_bert(\n",
    "    model,\n",
    "    test_dataloader,\n",
    "    mlm_criterion,\n",
    "    device\n",
    ")\n",
    "print('Valid MLM Loss: {test_mlm_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-26T15:49:54.63Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open('train_loss.txt', 'w') as traintxt:\n",
    "    traintxt.write(str(train_loss))\n",
    "\n",
    "with open('valid_loss.txt', 'w') as testtxt:\n",
    "    testtxt.write(str(valid_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_batch_loss.txt', 'w') as traintxt:\n",
    "    traintxt.write(str(train_batch_losses))\n",
    "\n",
    "with open('valid_batch_loss.txt', 'w') as testtxt:\n",
    "    testtxt.write(str(valid_batch_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-26T15:49:54.636Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.plot(train_batch_losses,label='train_loss',color='blue')\n",
    "plt.plot(valid_batch_losses,label='valid_loss',color='red')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Iteration vs Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6549191,
     "sourceId": 10582925,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6504967,
     "sourceId": 10583009,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
